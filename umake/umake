#!/usr/bin/python3.6
import time
from umake.colored_output import InteractiveOutput, bcolors, MINIMAL_ENV, format_text
from umake.colored_output import out
from umake.utils.timer import Timer

# from pyinstrument import Profiler
# profiler = Profiler()


class WorkerExit:
    pass

worker_exit_code = WorkerExit()

with Timer("done imports"):
    from subprocess import Popen, PIPE, check_output, TimeoutExpired
    from os.path import join
    from stat import S_ISDIR, S_IMODE
    import os
    import uuid
    import shutil
    import hashlib
    from enum import Enum, IntEnum, auto
    import pickle
    import igraph
    import pprint
    import threading
    from queue import Queue, Empty
    from collections import OrderedDict
    from itertools import chain
    import shutil
    import sys
    import urllib3
    import certifi
    import io
    import glob

    import umake.pywildcard as fnmatch
    from umake.cache.cache_mgr import CacheMgr
    from umake.cache.minio_cache import MinioCache
    from umake.exceptions import *
    from umake.utils.fs import fs_lock, fs_unlock, join_paths, get_size_KB
    from umake import config
    from umake.config import UMAKE_DB, UMAKE_MAX_WORKERS, ROOT, UMAKE_ROOT_DIR, UMKAE_TMP_DIR, UMAKE_BUILD_CACHE_DIR
    from umake.config import global_config, CONFIG_ENV_PREFIX
    from umake.cache.base_cache import MetadataCache


def byte_xor(ba1, ba2):
    return bytes([_a ^ _b for _a, _b in zip(ba1, ba2)])


class CmdExecuter:
    def __init__(self, target, sources, cmd):
        self.target = target
        self.sources = sources
        self.cmd: Cmd = cmd
        self.dep_files = None
        self.is_ok = False
        self.is_from_cache: CacheMgr.CacheType = CacheMgr.CacheType.NOT_CACHED

        # cache state
        """ in """
        self.deps_hash = None
        self.metadata_hash = None
        self.cmd_hash = None
        """ out """
        self.dep_files_hashes = dict()

    def _check_in_root(self, check_str: str):
        if check_str[0] == "/":
            if check_str.startswith("/tmp/") or check_str.startswith("/dev/") or check_str.startswith("/proc/") or \
               check_str.endswith(".pyc"):
                return None
            return check_str
        return join(self.cmd.cmd_root, check_str)

    def _parse_open(self, raw_path, args):
        """
        1234 open("/lib/x86_64-linux-gnu/libselinux.so.1", O_RDONLY|O_CLOEXEC) = 3
        1234 open("libc.so.6", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
        """
        rc_index = 5 if args[4] == "=" else 4
        rc = int(args[rc_index])
        if not rc >= 0:
            return None
        path = raw_path.split('"')[1]
        return self._check_in_root(path)

    def _parse_creat(self, raw_path, args):
        """
        1234 creat("/home/umake-user/debian-example/umake.deb", O_RDONLY|O_CLOEXEC) = 3
        1234 creat("tar_file.tar", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
        """
        rc_index = 5 if args[4] == "=" else 4
        rc = int(args[rc_index])
        if not rc >= 0:
            return None
        path = raw_path.split('"')[1]
        return self._check_in_root(path)

    def _parse_openat(self, raw_path, args):
        """
        21456 openat(AT_FDCWD, "/proc/sys/net/core/somaxconn", O_RDONLY|O_CLOEXEC) = 3
        23456 openat(AT_FDCWD, "casdasd", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3
        21460 openat(AT_FDCWD, "test/.dockerignore", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
        1168 openat(AT_FDCWD, "/usr/lib/x86_64-linux-gnu/libopcodes-2.30-system.so", O_RDONLY|O_CLOEXEC <unfinished ...>
        """
        if "<unfinished" == args[4]:
            out.print_fail(f"unfinished: {args[0]} {args[2]}")
            return None
        rc_index = 6 if args[5] == "=" else 5
        rc = int(args[rc_index])
        if not rc >= 0:
            return None
        path = args[2].split('"')[1]
        return self._check_in_root(path)

    def make(self, cache_mgr: CacheMgr, is_exiting):
        tmp_unique_name_full_path = join(UMKAE_TMP_DIR, str(uuid.uuid1()))
        with Timer(self.cmd.compile_show(), color=bcolors.WARNING) as timer:
            if global_config.verbose:
                timer.add_log_line(self.cmd.cmd)

            if self.target:
                cache_type = cache_mgr._get_cache(self.deps_hash, self.target)
                if cache_type > CacheMgr.CacheType.NOT_CACHED:
                    if cache_type == CacheMgr.CacheType.LOCAL:
                        timer.set_prefix("[LOCAL-CACHE]")
                    else:
                        cache_mgr._save_cache(self.deps_hash, self.target, local_only=True)
                        timer.set_prefix("[REMOTE-CACHE]")
                    self.is_ok = True
                    self.is_from_cache = cache_type
                    return
            strace_cmd = f"strace -o{tmp_unique_name_full_path} -f -e creat,open,openat /bin/bash -c '{self.cmd.cmd}'"
            self.proc = Popen(strace_cmd, env=os.environ, shell=True, stdout=PIPE, stderr=PIPE, cwd=self.cmd.cmd_root)

            while True:
                try:
                    stdout, stderr = self.proc.communicate(timeout=0.1)
                    break
                except TimeoutExpired:
                    if is_exiting() is False:
                        # exiting
                        self.proc.kill()
                        raise CleanExitErr()
                    out.curr_job = self.cmd.summarized_show()
                    out.update_bar()

            rc = self.proc.poll()
            stdout = stdout.decode("utf-8")
            stderr = stderr.decode("utf-8")
            if rc != 0:
                timer.add_log_line(format_text(f"{self.cmd.line}", bcolors.FAIL))
                timer.add_log_line(format_text(f"\t{self.cmd.cmd}", bcolors.BOLD))
                if stderr:
                    timer.add_log_line(format_text(f"\nError:\n\t\t{stderr}", ""))
                if stdout:
                    timer.add_log_line(format_text(f"\nt\t{stdout}", ""))

                raise CompilationFailedErr()
                # TODO: print here the source of the command
            else:
                if stdout:
                    timer.add_log_line(stdout)
                if stderr:
                    timer.add_log_line(stderr)

            self.dep_files = set()
            with open(tmp_unique_name_full_path) as strace_output:
                for line in strace_output.readlines():
                    args = line.split()
                    raw_path = args[1]
                    if raw_path.startswith('open('):
                        full_path = self._parse_open(raw_path, args)
                    elif raw_path.startswith('openat('):
                        full_path = self._parse_openat(raw_path, args)
                    elif raw_path.startswith('creat('):
                        full_path = self._parse_creat(raw_path, args)
                    else:
                        continue

                    if full_path is None:
                        continue
                    full_path = os.path.realpath(full_path)
                    # full_path = os.path.abspath(full_path)
                    if full_path in self.dep_files:
                        continue
                    try:
                        self.dep_files_hashes[full_path] = FileEntry.file_md5sum(full_path)
                    except (IsADirectoryError, FileNotFoundError):
                        # FileNotFoundError - might intermidiate file on filesystem that not exists
                        continue
                    self.dep_files.add(full_path)
            if self.target:
                if not self.target.issubset(self.dep_files):
                    timer.add_log_line(format_text(f"Target not generated:\n \tExpected {self.target}\n \tGot: {self.dep_files}", bcolors.FAIL))
                    raise TargetNotGeneratedErr()
                self.dep_files -= self.target

                deps_hash = self.cmd_hash
                for dep in self.dep_files:
                    deps_hash = byte_xor(deps_hash, self.dep_files_hashes[dep])
                cache_mgr._save_cache(deps_hash, self.target)
                timer.set_prefix("[CACHED]")

    def get_results(self):
        return self.dep_files, self.target


class FileEntry:

    class EntryType(Enum):
        GENERATED = auto()
        CMD = auto()
        FILE = auto()

    def __init__(self, full_path, entry_type, data=None):
        self.full_path = full_path
        self.entry_type: FileEntry.EntryType = entry_type
        self.mtime = 0
        self.md5sum = None
        self.is_modified = True
        self.data: Cmd = data
        self.dependencies_built = 0

        if entry_type not in [self.EntryType.CMD, self.EntryType.GENERATED]:
            self.update()
        if entry_type == self.EntryType.CMD:
            self.update_cmd()

    def init(self):
        self.dependencies_built = 0

    def set_modified(self, new_value: bool):
        self.is_modified = new_value

    def increase_dependencies_built(self, inc: int):
        self.dependencies_built += inc

    def update_cmd(self):
        self.md5sum = hashlib.sha1(self.full_path.encode("ascii")).digest()

    @staticmethod
    def file_md5sum(full_path):
        with open(full_path, "rb") as file_to_check:
            data = file_to_check.read()
            md5_returned = hashlib.sha1(data).digest()
            return md5_returned

    def update(self):
        modified = False
        stat = os.stat(self.full_path)
        if S_ISDIR(stat.st_mode):
            raise NotFileErr(f"failed to get info for {self.full_path}")
        new_mtime = int(stat.st_mtime * 100000)
        if new_mtime != self.mtime:
            new_md5sum = self.file_md5sum(self.full_path)
            if new_md5sum != self.md5sum:
                self.set_modified(True)
                modified = True

            self.md5sum  = new_md5sum
        self.mtime = new_mtime
        return modified

    def delete_fs(self):
        out.print_file_deleted(self.full_path, "DELETING")
        try:
            os.remove(self.full_path)
        except FileNotFoundError:
            pass

    def update_with_md5sum(self, new_md5sum):
        stat = os.stat(self.full_path)
        self.mtime = int(stat.st_mtime * 100000)
        self.md5sum = new_md5sum

    def __str__(self):
        return f"{self.full_path}: {self.data.conf_deps}"

    def __repr__(self):
        return f"['{self.full_path}': '{self.is_modified}']"


class Line:
    def __init__(self, filename, line_num, line):
        self.filename = filename
        self.line_num = line_num + 1
        self.line = line

    def __str__(self):
        return f"{self.filename}:{self.line_num}\n\t{self.line}"


class Cmd:

    def __init__(self, cmd, dep, manual_deps, target, line, cmd_root, source):
        self.cmd = cmd
        self.dep = dep
        self.manual_deps = manual_deps
        self.conf_deps = set(dep)
        self.target: set = target
        self.cmd_root = cmd_root
        self.source = source

        self.line: Line = line

    def compile_show(self):
        return " ".join(sorted(self.target))

    def summarized_show(self):
        return " ".join([os.path.basename(target) for target in sorted(self.target)])

    def update(self, other):
        self.line = other.line

    def __eq__(self, other):
        return self.cmd == other.cmd and \
               self.dep == other.dep and \
               self.target == other.target


class GraphDB:

    def __init__(self, db_version):
        self.nodes = dict()
        self.graph = igraph.Graph(directed=True)
        self.last_cmds = set()
        self.db_version = db_version

    def sub_graph_nodes(self, sub_nodes=[]):
        if sub_nodes == []:
            return [key for key, fentry in self.nodes.items() if fentry.entry_type != FileEntry.EntryType.CMD]

        vertecies = set()
        for node in sub_nodes:
            try:
                idx_node = self.graph.vs.find(node).index
                for x in self.graph.subcomponent(idx_node, mode="in"):
                    name = self.graph.vs[x]["name"]
                    if self.nodes[name].entry_type != FileEntry.EntryType.CMD:
                        vertecies.add(name)
            except ValueError:
                continue
        return vertecies

    def is_exists(self, node):
        return node in self.nodes

    def add_node(self, node, data: FileEntry):
        if node not in self.nodes:
            self.graph.add_vertex(node)
        self.nodes[node] = data

    def add_connection(self, from_node, to_node):
        if not self.graph.are_connected(from_node, to_node):
            self.graph.add_edge(from_node, to_node)

    def add_connections(self, connections):
        connections = [(from_node, to_node) for (from_node, to_node) in connections \
                            if not self.graph.are_connected(from_node, to_node)]
        self.graph.add_edges(connections)

    def remove_connections(self, connections):
        self.graph.delete_edges(connections)
        vertices_to_del = set()
        for dep, target in connections:
            try:
                vert = self.graph.vs.find(dep)
            except ValueError:
                # it might not be exists
                continue
            if vert.degree() == 0:
                vertices_to_del.add(dep)
        self.remove_node(vertices_to_del)

    def get_data(self, node) -> FileEntry:
        return self.nodes[node]

    def dump_graph(self):
        with open(UMAKE_DB, "wb") as db_file:
            pickle.dump(self, db_file, protocol=pickle.HIGHEST_PROTOCOL)

    def init(self):
        for node in self.get_nodes():
            fentry: FileEntry = self.get_data(node)
            # if fentry.entry_type == FileEntry.EntryType.CMD:
                # self.last_cmds.add(node)
            if fentry.entry_type == FileEntry.EntryType.FILE:
                fentry.init()

    @staticmethod
    def load_graph():
        pathname = os.path.realpath(__file__)
        db_file_entry = FileEntry(pathname, FileEntry.EntryType.FILE)
        db_version = db_file_entry.md5sum
        try:
            with open(UMAKE_DB, "rb") as db_file:
                data: GraphDB = pickle.load(db_file)
                return data
                # don't use it for now
                if data.db_version != db_version:
                    out.print_file_deleted(f"umake changed deleting db {UMAKE_DB}")
                    os.remove(UMAKE_DB)
                    return GraphDB(db_version)
                return data
        except FileNotFoundError:
            return GraphDB(db_version)

    def get_nodes(self, wanted_type=None):
        for name, node in self.nodes.items():
            if wanted_type:
                if node.entry_type == wanted_type:
                    yield name
            else:
                yield name

    def predecessors(self, node):
        for pred in self.graph.vs.find(node).predecessors():
            yield pred["name"]

    def successors(self, node):
        for succ in self.graph.vs.find(node).successors():
            yield succ["name"]

    def remove_node(self, nodes):
        if type(nodes) is not set:
            nodes = set([nodes])
        indecies = list()
        for node in nodes:
            del self.nodes[node]
            indecies.append(self.graph.vs.find(node).index)
        self.graph.delete_vertices(indecies)

    def topological_sort(self):
        return [self.graph.vs[i]["name"] for i in self.graph.topological_sorting()]

    def subgraph_topological_sort(self, sub_nodes):
        vertecies = set()
        for node in sub_nodes:
            try:
                idx_node = self.graph.vs.find(node).index
                vertecies.update(self.graph.subcomponent(idx_node, mode="in"))
            except ValueError:
                continue

        sub_graph = self.graph.subgraph(vertecies)
        return [sub_graph.vs[i]["name"] for i in sub_graph.topological_sorting()]


class CmdTemplate:

    def __init__(self, target, cmd, sources_fmt, deps_fmt, line_num, line, foreach, umakefile, cmd_root):
        self.targets_fmt = target
        self.cmd_fmt = cmd
        self.sources_fmt = sources_fmt
        self.deps_fmt = deps_fmt
        self.line = Line(umakefile, line_num, line)
        self.foreach = foreach
        self.root = cmd_root

        self.cmds = list()
        self.fs_files = set()

    def _iterate_file_glob(self, graph, fmt, all_targets):
        current_files = set()
        for full_path in glob.iglob(join_paths(self.root, fmt), recursive=True):
            if full_path not in all_targets:
                if not (graph.is_exists(full_path) and graph.get_data(full_path).entry_type == FileEntry.EntryType.GENERATED):
                    current_files.add(full_path)
        self.fs_files.update(current_files)
        return current_files

    def _create_foreach_cmd(self, files, manual_deps, all_targets, graph: GraphDB):
        for full_path in files:
            deps = set()
            deps.update(manual_deps)
            deps.add(full_path)
            basename = os.path.basename(full_path)
            dirname = os.path.dirname(full_path)
            noext = os.path.splitext(basename)[0]
            parent_dir = os.path.basename(dirname)
            if self.targets_fmt:
                targets = set()
                for target_fmt in self.targets_fmt:
                    target = target_fmt.format(filename=full_path,
                                                dir=dirname,
                                                basename=basename,
                                                noext=noext,
                                                parent_dir=parent_dir)
                    if not (full_path and target.startswith(ROOT)):
                        target = join_paths(self.root, target)
                    targets.add(target)
                    if target in all_targets:
                        raise RuntimeError(f"Failed parsing {self.line}\nTarget {target} already exists, two commands can't generate same target")
                all_targets.update(targets)
                cmd = self.cmd_fmt.format(filename=full_path,
                                        dir=dirname,
                                        basename=os.path.basename(full_path),
                                        noext=noext,
                                        target=target)
                self.cmds.append(Cmd(cmd, deps, manual_deps, targets, self.line, self.root, full_path))
            else:
                cmd = self.cmd_fmt.format(filename=full_path,
                                            dir=dirname,
                                            basename=os.path.basename(full_path),
                                            noext=noext)
                self.cmds.append(Cmd(cmd, deps, manual_deps, {}, self.line, self.root, full_path))

    def _find_target_in_all_targets(self, target_fmt, all_targets, found_targets):
        wild_card_exists = False
        target_found = False

        if not all_targets:
            return False

        if "*" in target_fmt:
            wild_card_exists = True

        # run fnmatch only on needed paths, if "*" no exists just search in set
        if wild_card_exists:
            for global_target in all_targets:
                if fnmatch.fnmatch(global_target, target_fmt):
                    found_targets.add(global_target)
                    target_found = True
        else:
            if target_fmt in all_targets:
                found_targets.add(target_fmt)
                target_found = True

        return target_found


    def create_cmds(self, graph: GraphDB, all_targets: set):
        full_path = None
        target = None
        manual_deps = set()

        for dep_fmt in self.deps_fmt:
            dep_fmt_fullpath = join_paths(self.root, dep_fmt)
            if self._find_target_in_all_targets(dep_fmt_fullpath, all_targets, manual_deps) is False:
                raise RuntimeError(f"{self.line}: manual dep '{dep_fmt}' is not exists as target in other commands")

        if self.foreach:
            for source_fmt in self.sources_fmt:
                generated_sources = set()
                src_fmt_fullpath = join_paths(self.root, source_fmt)

                self._find_target_in_all_targets(src_fmt_fullpath, all_targets, generated_sources)

                files = self._iterate_file_glob(graph, source_fmt, all_targets)
                files.update(generated_sources)
                self._create_foreach_cmd(files, manual_deps, all_targets, graph)

        else:
            deps = set()
            sources = set()
            generated_sources = set()

            deps.update(manual_deps)
            fs_sources = []
            for source_fmt in self.sources_fmt:
                is_found = False
                fs_sources =  self._iterate_file_glob(graph, source_fmt, all_targets)
                sources.update(fs_sources)
                if fs_sources:
                    is_found = True

                source_fmt_fullpath = join_paths(self.root, source_fmt)
                if not (self._find_target_in_all_targets(source_fmt_fullpath, all_targets, generated_sources) or is_found):
                    raise RuntimeError(f"[{source_fmt_fullpath}] {self.line}:\n \t\tsource mentioned in umakefile not exists")

            targets = set()
            if self.targets_fmt:
                basename = None
                noext = None
                dirname = None
                parent_dir = None

                if fs_sources:
                    full_path = sorted(fs_sources)[0]

                if full_path is not None:
                    basename = os.path.basename(full_path)
                    noext = os.path.splitext(basename)[0]
                    dirname = os.path.dirname(full_path)
                    parent_dir = os.path.basename(dirname)

                for target_fmt in self.targets_fmt:
                    target = target_fmt.format(filename=full_path,
                                            dir=dirname,
                                            basename=basename,
                                            noext=noext,
                                            parent_dir=parent_dir)
                    if not (full_path and target.startswith(ROOT)):
                        target = join_paths(self.root, target)
                    targets.add(target)
                target_exists = targets.intersection(all_targets)
                if target_exists:
                    raise RuntimeError(f"Target {target_exists} already exists, two commands can't generate same target")
            all_targets.update(targets)
            sources.update(generated_sources)
            filename = " ".join(sorted(sources))
            cmd = self.cmd_fmt.format(filename=filename,
                                      target=" ".join(sorted(targets)))
            deps.update(sources)
            deps.update(generated_sources)
            self.cmds.append(Cmd(cmd, deps, manual_deps, targets, self.line, self.root, full_path))


def find_between(string, token_start, token_end):
    state = "find_token_start"
    start_idx = None
    for idx, ch in enumerate(string):
        if state == "find_token_start":
            if ch == token_start:
                state = "find_token_end"
                start_idx = idx
        elif state == "find_token_end":
            if ch == token_end or idx == len(string) - 1:
                token = string[start_idx:idx+1].strip()
                if not token == "":
                    yield token
                state = "find_token_start"


class UMakeFileParser():
    """
    HELLO = 1
    : a.c > gcc {filename} -o {target} > {filename}.o
    : > gcc {filename} -o {target} > {filename}.o
    :foreach *.c > gcc {filename} -o {target} > {basename}.o
    :foreach | sdf > gcc {filename} -o {target} > {basename}.o
    """

    def __init__(self, filename):
        self.fielanme = filename
        self.cmds_template: [CmdTemplate] = []

        self.load_file(filename)
        self.globals_vars = dict()
        self.macros = dict()
        self.configs = dict()
        self.parsed_variants = {"default"}
        self.parse_file(filename)
        self._resolve_configs()
        self._check_variants_usage()

    def _get_config(self, config_name):
        env_config_name = f"{CONFIG_ENV_PREFIX}{config_name.upper()}"
        config_value = os.getenv(env_config_name, self.configs.get(config_name, False))
        try:
            del self.configs[config_name]
        except KeyError:
            pass
        return config_value
    
    def _check_variants_usage(self):
        not_exists_varaints = global_config.variant.difference(self.parsed_variants)
        if not_exists_varaints:
            raise CmdFailedErr(f"variant\s {not_exists_varaints} not exists, supported: {self.parsed_variants}")

    def _resolve_configs(self):
        config_value = self._get_config("remote_cache")
        if config_value:
            if global_config.remote_cache_config:
                remote_type, hostname, access_key, secret_pass, bucket_name, permissions = config_value.split()
                if permissions not in ['rw', 'ro']:
                    raise CmdFailedErr(f"not supported permission '{permissions}', supported are ['rw', 'ro']")
                if permissions == "rw":
                    global_config.remote_write_enable = True
                if remote_type == "minio":
                    global_config.remote_hostname = hostname
                    global_config.remote_access_key = access_key
                    global_config.remote_secret_key = secret_pass
                    global_config.remote_bucket = bucket_name
                    global_config.remote_cache_enable = True
                else:
                    raise CmdFailedErr(f"not supported remote cahce '{remote_type}''")
        
        config_value = self._get_config("local_cache_size")
        if config_value:
            global_config.local_cache_size = int(config_value)

        if self.configs:
            CmdFailedErr(f"unsupported config: '{self.configs}''")

    def load_file(self, filename):
        with open(filename, mode="r") as umakefile:
            return umakefile.read()

    def parse_file(self, umakefile, workdir=ROOT, in_variant=False, use_current_variant=False):
        if workdir is None:
            workdir = ROOT

        def should_line_parsing_stopped(in_variant, use_current_variant):
            return in_variant and not use_current_variant

        for line_num, line in enumerate(self.load_file(join(workdir, umakefile)).splitlines()):
            try:
                foreach = False
                deps_fmt = []
                source_fmt = []

                if line == "" or line[0] == "#":
                    if line == "" and in_variant:
                        in_variant = False
                        use_current_variant = False
                    continue

                if line[0] == ":":
                    if should_line_parsing_stopped(in_variant, use_current_variant):
                        continue
                    for macro_call in find_between(line, "!", ")"):
                        """ macro call - !gcc($x,$y,$z)"""
                        macro_name, macro_args_sent = macro_call.split("(", 1)
                        macro_args = self.macros[macro_name][0]
                        macro_args_sent = macro_args_sent[:-1].split(",")
                        macro_body = self.macros[macro_name][1]
                        macro_args_defaults = self.macros[macro_name][2]
                        if macro_args_sent == ['']:
                            macro_args_sent = []

                        for idx, in_macro_args in enumerate(macro_args):
                            try:
                                sent_arg = macro_args_sent[idx]
                            except IndexError:
                                sent_arg = macro_args_defaults[idx]

                            sent_arg = sent_arg.strip()
                            try:
                                if sent_arg == "":
                                    send_arg_value = ""
                                elif sent_arg[0] == "$":
                                    send_arg_value = self.globals_vars[sent_arg]
                                else:
                                    send_arg_value = sent_arg
                            except KeyError:
                                raise CmdFailedErr(f"{umakefile}:{line_num} macro {macro_name} called with not exists arg: {macro_args_sent}")
                            macro_body = macro_body.replace(in_macro_args, send_arg_value)

                        line = line.replace(macro_call, macro_body)
                    sources_cand, cmd_fmt, targets_fmt = line.split(">")
                    sources_cand = sources_cand.split()
                    targets_fmt = targets_fmt.split()
                    if len(sources_cand) == 1:
                        pass
                        # No sources
                    else:
                        deps_index = len(sources_cand)
                        try:
                            deps_index = sources_cand.index("|")
                            deps_fmt = sources_cand[deps_index + 1:]
                            # resolve $var
                            resolved_manual_deps = []
                            remove_indexs = []
                            for idx, manual_dep in enumerate(deps_fmt):
                                if manual_dep[0] == "$":
                                    resolved_manual_deps.extend(self.globals_vars[manual_dep].split())
                                    remove_indexs.append(idx)
                            for idx in remove_indexs:
                                del deps_fmt[idx]
                            deps_fmt.extend(resolved_manual_deps)
                        except ValueError:
                            pass

                        foreach = True if sources_cand[0] == ":foreach" else False
                        source_fmt = sources_cand[1:deps_index]
                    if workdir:
                        if not os.path.isdir(workdir):
                            CmdFailedErr(f"path is not directory {workdir}")
                        cmd_root = workdir
                    else:
                        cmd_root = ROOT
                    self.cmds_template.append(CmdTemplate(targets_fmt, cmd_fmt.strip(), source_fmt,
                                                          deps_fmt, line_num, line, foreach, umakefile, cmd_root))
                elif line[0] == "!":
                    if should_line_parsing_stopped(in_variant, use_current_variant):
                        continue
                    args = list()
                    defaults = list()
                    """ !compile-c(a, b, c=$sdf) : gcc -c -fPIC {filename} -o {target} > {dir}/{noext}.o """
                    macro_decl, macro_body = line.split(":", 1)
                    macro_decl = macro_decl.replace(" ", "")
                    macro_name, marco_args = macro_decl.strip().split("(")
                    marco_args = marco_args[:-1]  # remove ending )
                    marco_args = marco_args.split(",")
                    if marco_args != ['']:
                        for arg in marco_args:
                            # get defaults
                            try:
                                arg_name, arg_default = arg.split("=")
                            except ValueError:
                                arg_name = arg
                                arg_default = ""
                            args.append(f"${arg_name}")
                            defaults.append(arg_default)
                    self.macros[macro_name] = (args, macro_body.strip(), defaults)
                elif line[0] == "$":
                    if should_line_parsing_stopped(in_variant, use_current_variant):
                        continue
                    try:
                        var_name, var_body = line.split("+=", 1)
                        for var_to_replace in find_between(var_body, "$", " "):
                            var_body = var_body.replace(var_to_replace, self.globals_vars[var_to_replace])
                        try:
                            self.globals_vars[var_name.strip()] += f" {var_body.strip()}"
                        except KeyError:
                            raise CmdFailedErr(f"{line_num}: {line}, var {var_name.strip()} was not declared")
                    except ValueError:
                        var_name, var_body = line.split("=", 1)
                        for var_to_replace in find_between(var_body, "$", " "):
                            var_body = var_body.replace(var_to_replace, self.globals_vars[var_to_replace])
                        self.globals_vars[var_name.strip()] = var_body.strip()
                elif line[0] == "[":
                    if line[-1] != "]":
                        raise CmdFailedErr(f"{line_num}: {line} \n can't parse this line")
                    if should_line_parsing_stopped(in_variant, use_current_variant):
                        continue
                    config_name, config_value = line[1:-1].split(":", 1)
                    config_name = config_name.strip()
                    config_value = config_value.strip()
                    if config_name == "variant":
                        if in_variant:
                            raise CmdFailedErr(f"{line_num}: {line} \n cannot configure variant in variant")
                        in_variant = True
                        self.parsed_variants.add(config_value)
                        if config_value in global_config.variant:
                            use_current_variant = True
                    elif config_name == "workdir":
                        if config_value == "/":
                            workdir = None
                        else:
                            workdir = join(ROOT, config_value)
                    elif config_name == "include":
                        self.parse_file(config_value, workdir, in_variant, use_current_variant)
                    else:
                        self.configs[config_name] = config_value
                else:
                    raise RuntimeError(f"{line_num}: {line} \n can't parse this line")


            except:
                out.print_fail(f"ERROR failed to parse UMakefile")
                out.print_fail(f"{umakefile}:{line_num}")
                out.print_fail(f"   {line}")
                raise


class UMake:

    def __init__(self):
        self.graph = None
        self.worker_continue = True  # should wor
        self.worker_threads = list()

    def _init_build_dirs(self):
        shutil.rmtree(UMKAE_TMP_DIR, ignore_errors=True)
        os.makedirs(UMKAE_TMP_DIR, exist_ok=True)
        os.makedirs(UMAKE_BUILD_CACHE_DIR, exist_ok=True)

    def _start_executer_threads(self):
        self.jobs_queue = Queue() # CmdExecuter
        self.done_queue = Queue()
        self.n_jobs = 0
        for _ in range(UMAKE_MAX_WORKERS):
            exec_thread = threading.Thread(target=self.executer_thread)
            self.worker_threads.append(exec_thread)
            exec_thread.start()

    def _get_file_entry(self, full_path):
        return FileEntry(full_path, FileEntry.EntryType.FILE)

    def _is_in_blacklist(self, path):
        if "/." in path:
            return True
        if path[0] == '.':
            return True
        return False

    def _remove_generated_from_graph(self, deleted_gen, delete_set):
        out.print_file_deleted(deleted_gen)
        fentry = self.graph.get_data(deleted_gen)
        fentry.set_modified(True)
        for pred_deleted_gen in self.graph.predecessors(deleted_gen):
            pred_fentry = self.graph.get_data(pred_deleted_gen)
            pred_fentry.set_modified(True)
        # self.graph.remove_node(deleted_gen)
        delete_set.add(deleted_gen)
        fentry.delete_fs()

    def _remove_file_from_graph(self, delete_file, delete_set):
        succs = list(self.graph.successors(delete_file))
        for succ in succs:
            fentry = self.graph.get_data(succ)
            fentry.set_modified(True)

        out.print_file_deleted(delete_file)
        # self.graph.remove_node(delete_file)
        delete_set.add(delete_file)

    def scan_fs(self):
        with Timer("done filesystem scan"):
            graph_fs = self.graph.sub_graph_nodes(global_config.targets)
            deleted_set = set()
            for f in graph_fs:
                if f in deleted_set:
                    continue
                fentry: FileEntry = self.graph.get_data(f)
                try:
                    if fentry.update():
                        for pred_deleted_gen in self.graph.predecessors(f):
                            pred_entry = self.graph.get_data(pred_deleted_gen)
                            pred_entry.set_modified(True)
                        out.print_file_updated(f)
                except FileNotFoundError:
                    if fentry.entry_type == FileEntry.EntryType.GENERATED:
                        self._remove_generated_from_graph(f, deleted_set)
                    elif fentry.entry_type == FileEntry.EntryType.FILE:
                        self._remove_file_from_graph(f, deleted_set)
                    else:
                        raise RuntimeError(f"trying to delete not file but type is {fentry.entry_type}. how can it be???")
            self.graph.remove_node(deleted_set)

    def _graph_remove_cmd_node(self, cmd: Cmd, connection):
        delete = set()
        for conn in connection:
            if cmd.cmd in conn:
                delete.add(conn)
        for del1 in delete:
            connection.remove(del1)
        self.graph.remove_node(cmd.cmd)

    def _graph_add_cmd_node(self, cmd: Cmd, connections: set):
        fentry_cmd = FileEntry(cmd.cmd, FileEntry.EntryType.CMD, cmd)
        self.graph.add_node(cmd.cmd, fentry_cmd)

        for target in cmd.target:
            self.graph.add_node(target, FileEntry(target, FileEntry.EntryType.GENERATED, cmd))
            connections.add((cmd.cmd, target))
        for dep in cmd.dep:
            connections.add((dep, cmd.cmd))

    def _graph_update_cmd_node(self, old_cmd: Cmd, new_cmd: Cmd, fentry_cmd: FileEntry, connections: set):
        if old_cmd != new_cmd:
            self._graph_remove_cmd_node(old_cmd, connections)
            self._graph_add_cmd_node(new_cmd, connections)
        else:
            old_cmd.update(new_cmd)

        for target in new_cmd.target:
            if not self.graph.is_exists(target):
                self.graph.add_node(target, FileEntry(target, FileEntry.EntryType.GENERATED, new_cmd))
            connections.add((new_cmd.cmd, target))

        for dep in new_cmd.dep:
            connections.add((dep, new_cmd.cmd))

    def parse_cmd_files(self):
        with Timer("done parsing UMakefile"):
            last_cmds = self.graph.last_cmds
            all_targets = set()
            UMakefile = join(ROOT , "UMakefile")
            umakefile = UMakeFileParser(UMakefile)

            cmd_template: CmdTemplate
            cmds = set()
            for cmd_template in umakefile.cmds_template:
                cmd_template.create_cmds(self.graph, all_targets)
                cmd: Cmd
                for f in cmd_template.fs_files:
                    if not self.graph.is_exists(f):
                        new_fentry = self._get_file_entry(f)
                        self.graph.add_node(f, new_fentry)
                for cmd in cmd_template.cmds:
                    cmds.add(cmd.cmd)
            removed_cmds = last_cmds.difference(cmds)

            delete_nodes = set()
            for remove_cmd in removed_cmds:
                remove_cmd_list = list(self.graph.successors(remove_cmd))
                for target_file in remove_cmd_list:
                    if self.graph.is_exists(target_file):
                        succseccors = self.graph.successors(target_file)
                        for succ in succseccors:
                            fentry = self.graph.get_data(succ)
                            fentry.data.dep.remove(target_file)

                        target_fentry = self.graph.get_data(target_file)
                        target_fentry.delete_fs()
                        delete_nodes.add(target_file)
                        # self.graph.remove_node(target_file)
                # self.graph.remove_node(remove_cmd)
                delete_nodes.add(remove_cmd)
            self.graph.remove_node(delete_nodes)

            connections = set()
            for cmd_template in umakefile.cmds_template:
                for cmd in cmd_template.cmds:
                    if cmd.cmd in removed_cmds:
                        continue
                    if cmd.cmd in last_cmds:
                        fentry_cmd: FileEntry = self.graph.get_data(cmd.cmd)
                        self._graph_update_cmd_node(fentry_cmd.data, cmd, fentry_cmd, connections)
                    else:
                        self._graph_add_cmd_node(cmd, connections)
            self.graph.add_connections(connections)
            self.graph.last_cmds = cmds
            # check target request exists
            if global_config.targets:
                all_targets_exists = any(target in all_targets for target in global_config.targets)
                if not all_targets_exists:
                    raise CmdFailedErr(f"target not exist {global_config.targets}")

    def executer_thread(self):
        cache_mgr = CacheMgr()
        while self.worker_continue:
            executer: CmdExecuter
            executer = self.jobs_queue.get()

            if type(executer) is WorkerExit:
                return

            out.n_active_workers.inc()
            out.curr_job = executer.cmd.summarized_show()
            try:
                executer.make(cache_mgr, lambda: self.worker_continue)
                executer.is_ok = True
            except (CompilationFailedErr, TargetNotGeneratedErr, CleanExitErr) as e:
                executer.is_ok = False
                self._send_exits()
            except Exception as e:
                import traceback
                traceback.print_exc()
                out.print(e)
                executer.is_ok = False
                self._send_exits()
            out.n_active_workers.dec()

            self.done_queue.put(executer)


    def _handle_done(self, add_conns_out, del_conns_out):
        execucter: CmdExecuter
        execucter = self.done_queue.get()
        self.n_jobs -= 1
        if execucter.is_ok is False:
            raise CmdFailedErr()

        deps, targets = execucter.get_results()
        node = execucter.cmd.cmd

        node_entry: FileEntry = self.graph.get_data(node)
        node_entry.set_modified(False)
        out.n_works_done += 1
        if execucter.is_from_cache:
            if execucter.is_from_cache == CacheMgr.CacheType.LOCAL:
                out.n_local_hits +=1
            if execucter.is_from_cache == CacheMgr.CacheType.REMOTE:
                out.n_remote_hits +=1
        conns = []
        preds = set(self.graph.predecessors(node))
        for dep in deps:
            if self.graph.is_exists(dep):
                dep_fentry: FileEntry = self.graph.get_data(dep)
                if dep_fentry.entry_type == FileEntry.EntryType.GENERATED and dep not in preds:
                    out.print_fail(f"at line: {execucter.cmd.line}")
                    out.print_fail(f"\t'{node}' :\n have generated dependency '{dep}'")
                    out.print_fail(f"from line: {dep_fentry.data.line}")
                    out.print_fail(f"add to line: {execucter.cmd.line}")
                    out.print_fail(f"{dep} as a manual dependency")
                    raise DepIsGenerated()
                if execucter.dep_files_hashes:
                    dep_fentry.update_with_md5sum(execucter.dep_files_hashes[dep])
                else:
                    dep_fentry.update()
            else:
                try:
                    fentry = FileEntry(dep, FileEntry.EntryType.FILE)
                    fentry.set_modified(False)
                except NotFileErr as e:
                    out.print(e)
                    continue
                self.graph.add_node(dep, fentry)

            if not self.graph.graph.are_connected(dep, node):
                conns.append((dep, node))
        # with Timer(f"connections {execucter.cmd.summarized_show()}: {len(conns)}", color=bcolors.FAIL):
        #     self.graph.add_connections(conns)
        add_conns_out.extend(conns)

        del_cons = set()
        # kepp user conigured deps
        preds = preds - execucter.cmd.dep
        deleted_autodeps = preds - deps
        for del_autodep in deleted_autodeps:
            del_cons.add((del_autodep, node))
        # self.graph.remove_connections(del_cons)
        del_conns_out.extend(del_cons)

        for target in targets:
            target_node = self.graph.get_data(target)
            target_node.increase_dependencies_built(-1)
            target_node.update()

        if targets and not execucter.is_from_cache:
            self._set_deps_hash(node_entry, execucter)

        if self.n_jobs == 0:
            return False
        else:
            return True

    def _calc_hash(self, cmd_hash, deps) -> bytes:
        tree_hash = cmd_hash
        for dep in deps:
            with Timer(f"WARNING: hash for {dep} took longer then usueal", threshold=0.05, color=bcolors.FAIL):
                try:
                    fentry: FileEntry = self.graph.get_data(dep)
                except KeyError:
                    fentry = self._get_file_entry(dep)
                    fentry.set_modified(False)
                    self.graph.add_node(dep, fentry)
                    # out.print_file_add(dep)
                tree_hash = byte_xor(tree_hash, fentry.md5sum)
        return tree_hash

    def _set_deps_hash(self, node_entry, execucter: CmdExecuter):
        metadata_hash = execucter.metadata_hash
        self.cache_mgr.save_cache(metadata_hash, MetadataCache(execucter.dep_files))

    def _get_deps_hash(self, node_entry):
        metadata_hash: bytes = self._calc_hash(node_entry.md5sum, node_entry.data.conf_deps)
        if not node_entry.data.target:
            return None, None, metadata_hash
        try:
            metadata = self.cache_mgr.open_cache(metadata_hash)
            deps_hash = self._calc_hash(node_entry.md5sum, metadata.deps)
            return deps_hash, metadata.deps, metadata_hash
        except FileNotFoundError:
            return None, None, metadata_hash

    def execute_graph(self):
        add_conns = []
        del_conns = []
        if global_config.targets:
            top_sort = self.graph.subgraph_topological_sort(global_config.targets)
        else:
            top_sort = list(self.graph.topological_sort())
        for node in top_sort:
            node_entry: FileEntry = self.graph.get_data(node)
            if not node_entry.is_modified:
                continue
            while node_entry.dependencies_built > 0:
                self._handle_done(add_conns, del_conns)

            successors = set()
            for succ in self.graph.successors(node):
                succ_node = self.graph.get_data(succ)
                succ_node.set_modified(True)
                if node_entry.entry_type == FileEntry.EntryType.CMD:
                    succ_node.increase_dependencies_built(1)
                successors.add(succ)
            if node_entry.entry_type == FileEntry.EntryType.CMD:
                deps_hash, cached_deps, metadata_hash = self._get_deps_hash(node_entry)
                execucter = CmdExecuter(successors, "", node_entry.data)

                execucter.cmd_hash = node_entry.md5sum
                execucter.metadata_hash = metadata_hash
                execucter.deps_hash = deps_hash
                execucter.dep_files = cached_deps
                self.jobs_queue.put(execucter)
                self.n_jobs += 1
            else:
                node_entry.set_modified(False)

        if self.n_jobs:
            while self._handle_done(add_conns, del_conns):
                pass

        # start terminating threads
        self._send_exits()

        self.graph.add_connections(add_conns)
        self.graph.remove_connections(del_conns)

    def dump_graph(self):
        with Timer("done saving graph"):
            self.graph.dump_graph()

    def load_graph(self):
        with Timer("done loading graph"):
            self.graph = GraphDB.load_graph()
            self.graph.init()

    def cache_gc(self):
        self.cache_mgr.gc()

    def create_compilation_database(self):
        """
        Create a compilation database based on the graph of the project.
        Example entry for compilation database:
        [
            { "directory": "/home/user/llvm/build",
              "command": "/usr/bin/clang++ -Irelative -DSOMEDEF=\"With spaces, quotes and \\-es.\" -c -o file.o file.cc",
              "file": "file.cc" },
            ...
        ]

        For more details on compilation databases refer to:
        https://clang.llvm.org/docs/JSONCompilationDatabase.html
        """
        cmds = []
        for node in self.graph.nodes.values():
            # We only care about the commands that are executed, skip eveything else.
            if node.entry_type != FileEntry.EntryType.CMD:
                continue

            cmds.append({
                'directory': '/',
                'command': str(node.data.cmd),
                'file': str(node.data.source),
            })

        import json
        with open('compile_commands.json', 'w') as db_file:
            json.dump(cmds, db_file)

    def _send_exits(self):
        if self.worker_continue is False:
            return
        self.worker_continue = False
        for _ in range(UMAKE_MAX_WORKERS):
            self.jobs_queue.put(worker_exit_code)

    def _exit_nicelly(self):
        # stop all executers
        self._send_exits()
        with Timer("done clean exit"):
            for thread in self.worker_threads:
                thread.join(timeout=1)

    def run(self):
        fd, lock_path = fs_lock(UMAKE_ROOT_DIR)
        if fd == None:
            out.print_fail(f"another umake is running!, if you sure it's not running remove {UMAKE_ROOT_DIR}.lock")
            os.sys.exit(-1)
            return

        try:
            self._init_build_dirs()
            self.load_graph()
            self.scan_fs()
            # profiler.start()
            self.parse_cmd_files()
            # profiler.stop()
            try:
                self._start_executer_threads()
                self.cache_mgr = CacheMgr()
                self.execute_graph()
            finally:
                self._exit_nicelly()
            # print(profiler.output_text(color=True, show_all=True))
            self.dump_graph()
            if global_config.compile_commands:
                self.create_compilation_database()
            self.cache_gc()
            out.update_bar(force=True)
        except CmdFailedErr as e:
            out.print_fail(e)
            os.sys.exit(-1)
        except Exception as e:
            import traceback
            traceback.print_exc()
            out.print(e)
            os.sys.exit(-1)
        finally:
            fs_unlock(fd, lock_path)

    def show_target_details(self, target):
        target_fentry: FileEntry
        target_fentry = self.graph.get_data(target)

        cmd = target_fentry.data.cmd
        all_deps = set(self.graph.predecessors(cmd))

        configured_deps = sorted(target_fentry.data.dep - target_fentry.data.manual_deps)
        manual_deps = sorted(target_fentry.data.manual_deps)
        auto_deps = all_deps - target_fentry.data.dep
        auto_dep_in_project = set()
        for auto_dep in auto_deps:
            if fnmatch.fnmatch(auto_dep, ROOT + "/*"):
                auto_dep_in_project.add(auto_dep)
        global_auto_deps = sorted(auto_deps - auto_dep_in_project)
        auto_dep_in_project = sorted(auto_dep_in_project)
        successors = set(self.graph.successors(target))

        if global_config.json_file:
            with open(global_config.json_file, "w") as f:
                import json
                out_json = dict()
                deps = dict()
                deps["configured"] = configured_deps
                deps["manual"] = manual_deps
                deps["auto_in"] = auto_dep_in_project
                deps["auto_global"] = global_auto_deps
                out_json["target"] = target
                out_json["deps"] = deps
                out_json["cmd"] = target_fentry.data.cmd

                f.write(json.dumps(out_json))
        else:
            out.print_colored(f"{target}: [{target_fentry.md5sum.hex()}]", bcolors.HEADER)
            print("\tdeps:")
            for idx, dep in enumerate(configured_deps):
                print(f"\t\t{idx:4} {dep:70} [{self.graph.get_data(dep).md5sum.hex()}]")
            for idx, dep in enumerate(manual_deps):
                out.print_colored(f"\t\t{idx:4} {dep:70} [{self.graph.get_data(dep).md5sum.hex()}]", bcolors.WARNING)
            for idx, dep in enumerate(auto_dep_in_project):
                out.print_colored(f"\t\t{idx:4} {dep:70} [{self.graph.get_data(dep).md5sum.hex()}]", bcolors.OKGREEN)
            for idx, dep in enumerate(global_auto_deps):
                out.print_colored(f"\t\t{idx:4} {dep:70} [{self.graph.get_data(dep).md5sum.hex()}]", bcolors.OKBLUE)
            print()
            print("\tsuccessors targets:")
            for succ in sorted(successors):
                succ_targets = " ".join(sorted(set(self.graph.successors(succ))))
                print(f"\t\t{succ_targets}")
            print()
            print("\tcmd:")
            print(f'\t\t{target_fentry.data.cmd}')
            print()
            print(f"\tUMakdile:{target_fentry.data.line.line_num}\n\t\t{target_fentry.data.line.line}")

    def show_all_targets(self):
        generated = self.graph.get_nodes(FileEntry.EntryType.GENERATED)
        for idx, target in enumerate(sorted(generated)):
            print(f'{idx:4} {target.replace(ROOT + "/", "")}')

    def clean(self):
        generated = self.graph.get_nodes(FileEntry.EntryType.GENERATED)
        with Timer("done cleaning targets"):
            for idx, target in enumerate(sorted(generated)):
                out.print_file_deleted(target, "DELETING")
                try:
                    os.remove(target)
                except FileNotFoundError:
                    pass
        with Timer("done cleaning local cache"):
            fd, lock_path = fs_lock(UMAKE_BUILD_CACHE_DIR)
            try:
                for root, dirs, files in os.walk(UMAKE_BUILD_CACHE_DIR):
                    for f in files:
                        os.unlink(os.path.join(root, f))
                    for d in dirs:
                        shutil.rmtree(os.path.join(root, d))
            finally:
                fs_unlock(fd, lock_path)
        with Timer("done deleting build db"):
            try:
                shutil.rmtree(UMAKE_ROOT_DIR, ignore_errors=True)
            except FileNotFoundError:
                pass

    def show_targets_details(self, target):
        generated = self.graph.get_nodes(FileEntry.EntryType.GENERATED)
        for graph_target in sorted(generated):
            if fnmatch.fnmatch(graph_target, target):
                self.show_target_details(graph_target)

    def show_target_details_run(self, targets):
        self.load_graph()
        self.parse_cmd_files()
        for target in targets:
            self.show_targets_details(target)

    def show_parsed_umakefile(self):
        self.load_graph()
        UMakefile = join(ROOT , "UMakefile")
        umakefile = UMakeFileParser(UMakefile)

        cmd_template: CmdTemplate
        all_targets = set()
        for cmd_template in umakefile.cmds_template:
            cmd_template.create_cmds(self.graph, all_targets)
            print(cmd_template.cmd_fmt)
            for cmd in cmd_template.cmds:
                print(f"\t{cmd.cmd}")

    def parse_args(self):
        import argparse
        parser = argparse.ArgumentParser()

        parser.add_argument('targets', type=str, nargs="*",
                            help='target path')

        parser.add_argument('-d', '--details', action='store_true',
                            help='details about the target')

        parser.add_argument('--json', action='store', dest='json_file',
                            help='output as json')

        parser.add_argument('--show-all-targets', action='store_true', dest="show_all_targets",
                            help="show all targets configured in UMakefile")

        parser.add_argument('--show-parsed-umakefile', action='store_true', dest="show_parsed_umakefile",
                            help="show parsed umakefile")

        parser.add_argument('--no-remote-cache', action='store_true', dest="no_remote_cache",
                            help="don't use remote cache")

        parser.add_argument('--no-local-cache', action='store_true', dest="no_local_cache",
                            help="don't use local cache")

        parser.add_argument('--remote-cache-stats', action='store_true', dest="remote_cache_stats",
                            help="show stats of remote cache")

        parser.add_argument('--remote-cache-delete', action='store_true', dest="remote_cache_delete",
                            help="WARNING: delete all remote cache objects")

        parser.add_argument('-v', '--variant', 
                            action='append', 
                            type=str,
                            dest="variant",
                            default=[],
                            help="compile with diffrent variants")

        parser.add_argument('--clean', action='store_true', dest="clean",
                            help="clean umake file, with all targets")
        
        parser.add_argument('--verbose', action='store_true', dest="verbose",
                            help="show verbose compilation")

        parser.add_argument('--compile-commands', action='store_true', dest="compile_commands",
                            help="Create compile_commands.json file with info on the build")

        args = parser.parse_args()

        global_config.compile_commands = args.compile_commands

        if args.verbose:
            global_config.verbose = args.verbose

        if args.json_file:
            global_config.json_file = args.json_file

        if args.no_remote_cache:
            global_config.remote_cache_config = False

        if args.no_local_cache:
            global_config.local_cache = False

        if args.remote_cache_stats:
            mc = MinioCache()
            mc.get_cache_stats()
            return

        if args.remote_cache_delete:
            mc = MinioCache()
            mc.clear_bucket()
            return

        if args.show_all_targets:
            self.load_graph()
            self.show_all_targets()

        if args.variant:
            global_config.variant = set(args.variant)
            out.variant = " ".join(global_config.variant)

        if args.show_parsed_umakefile:
            self.show_parsed_umakefile()
            return

        if args.clean:
            self.load_graph()
            self.clean()
            return
        
        if args.details:
            args.targets = [join(ROOT, t) + "**" for t in args.targets]
            self.show_target_details_run(args.targets)
        else:
            args.targets = [join(ROOT, t) for t in args.targets]
            global_config.targets = args.targets
            self.run()

    def start(self):
        if len(sys.argv) == 1:
            self.run()
        else:
            self.parse_args()
        out.destroy()



umake = UMake()
umake.start()

# print(profiler.output_text(color=True))
